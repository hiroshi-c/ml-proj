Machine Learning applied to Weight Lifting Exercises Dataset
============================================================
By Hiroshi Ohno

August 22, 2015

Executive Summary
=================
This report explains a machine learning model built for Weight Lifting Exercises Dataset. This explains how the model was built and then evaluates the performance of final model.

The final model chosen uses Random Forest alogorithm. Its total accuracy is between 98.91% and 99.48% with 95% confidence interval.

Notes about R code in this report
=================================
This report is written by R markdown and processed by knitter.
A few lines of R code which are directly unrelated may become needed.

This report uses caret package for various machine learning operations.
```{r warning=FALSE, message=FALSE}
library(caret)
```

Parallel operation is applied by using 'doParallel' package.
```{r warning=FALSE, message=FALSE}
library(doParallel)
registerDoParallel(cores=4)
```

This report has some commented out lines which read '.rds' files.
Those '.rds' files are the models generated by previous run of this R script.
Those were used in the middle of authoring this report to skip the actual training steps.
It is not used for the final report, but kept as comment for future revision of this report.

Input Data
==========
Read source dataset from given CSV file. as.is=T is used because a few columns are interpreted as factors by default even though they are numbers.
On the other hand, classification column (classe) is actually a factor, so it is converted as such explicitly.
The source has missing observeations both as "NA" and just empty string.
```{r}
data <- read.csv(file="pml-training.csv", header=TRUE, sep=",", quote="\"", na.strings=c("NA",""), as.is = T)
data$classe <- as.factor(data$classe)
```

Remove a few colums which are not the observed value. Those are sequence number, user name, timestamp related data.
```{r}
data <- subset(data, select=-c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))
```

A few columns have many NA's. Remove columns that have half or more of NA observations.
```{r}
goodColumns <- (colSums(is.na(data)) < nrow(data)/2)
data <- data[ , goodColumns]
```

At this moemnt, there is no NA remaining.
```{r}
sum(is.na(data)) == 0
```

It is reconfirmed that all the colums except 'classe' are numeric or integer type.
```{r}
sum(lapply(data, class) %in% c("numeric", "integer")) == (ncol(data) - 1)
```

Data division
=============
Since the number of observations are nearly 20000 which look large enough, the input data is simply divided into 3 groups. 60% for training, 20% for cross validation, and 20% for final testing. Set the seed for reproducibility.
```{r}
set.seed(2415)
inTraining <- createDataPartition(y=data$classe, p=0.6, list=FALSE)
training <- data[inTraining, ]
remaining <- data[-inTraining, ]
inValidation <- createDataPartition(y=remaining$classe, p=0.5, list=FALSE)
validation <- remaining[inValidation, ]
testing <- remaining[-inValidation, ]
sprintf("Number of samples: training=%d, validation=%d, testing=%d", nrow(training), nrow(validation), nrow(testing))
```


Data standardization
====================
Various algorithms will be tested in the next section.
Some algorithm may not need this, but very basic standadization is applied to the data.
Training data is transformed so that the mean becomes zero and standard deviation becomes one.
All other data are also transformed by the same scaling as training data.
As preProcess does not work with factor columns, 'classe' column needs to be removed and then concatenated later.
```{r}
preprocess <- preProcess(subset(training, select=-c(classe), method=c("center", "scale")))
trainingP <- predict(preprocess, subset(training, select=-c(classe)))
trainingP <- cbind(training$classe, trainingP)
names(trainingP)[1] <- "classe"
validationP <- predict(preprocess, subset(validation, select=-c(classe)))
validationP <- cbind(validation$classe, validationP)
names(validationP)[1] <- "classe"
testingP <- predict(preprocess, subset(testing, select=-c(classe)))
testingP <- cbind(testing$classe, testingP)
names(testingP)[1] <- "classe"
```

Parameter distribution after the standalization looks like this chart.
There are several outlier points and log scaling could be applied.
```{r, fig.width=10, fig.height=8}
featurePlot(x=trainingP[ , -1], y=trainingP[ , 1], plot="box", auto.key = list(columns = 2))
```

At this moment, further standadization is not made and all variables will be used for model consturction.

Algorithm Selection
===================
8 different machine learning algorithms are trained by training data.
All algorithms use default training control parameter set of caret package.
Then, accuracy against cross validation data is measured for each result.
These algorithms are those which are mentioned by the lecture videos.
Actually 5 othes (glm, bagEarth, ada, gamBoost, bagFDA) are tried, but those failed either complaining bout the source data or due to out of memory.
```{r warning=FALSE, message=FALSE}
models = c("lda", "gbm", "mda", "rpart", "rf", "treebag", "nb", "LogitBoost")
for (model in models) {
  ### Training the model
  filename <- paste("cache/fit-", model, ".rds", sep="")
  #fit <- train(classe ~ ., data=trainingP, method=model)
  #saveRDS(fit, file=filename)
  fit <- readRDS(filename)
  ### Predict for cross validation data 
  predictionValidatoin <- predict(fit, newdata=validationP)
  overall <- confusionMatrix(predictionValidatoin, validationP$classe)$overall
  print(sprintf("Model %s: accuracy: %.4f  95%% CI: %.4f - %.4f", model, overall[1], overall[3], overall[4]))
  ### Cleanup to avoid out of memory
  rm(fit, predictCrossValidation)
}
```
As Random Forest achieved the highest accuracy, it is chosen for the final model.

Model optimization
==================
Random Forest algorithm with caret package has one tuing parameter called mtrf, which is the number of randomly selected predictors. Here, mtry is set from 1 to 30 and training - cross validation process is conducted as same as prior section.

'result' data frame stores the accuracy of all iterations both against training data and cross validation data. Construction of this data frame needed some complicated code, but it is implementation detail and can be ignored for the purpose of understanding the model construction.
```{r warning=FALSE, message=FALSE}
### Initialize result DF
result<-data.frame(c(1,1), c(1,1), c("training", "validation"))
for (mtry in seq(1, 30, 1))
{
  ### Training the model
  filename <- paste("cache/fit-rf-mtry-", mtry, ".rds", sep="")
  #fit <- train(classe ~ ., data=trainingP, method="rf", tuneGrid = data.frame(.mtry = mtry))
  #saveRDS(fit, file=filename)
  fit <- readRDS(filename)
  ### Predict for training data
  predictTraining <- predict(fit, newdata=trainingP)
  result <- rbind(result, c(mtry, confusionMatrix(predictTraining, trainingP$classe)$overall[1], "training"))
  ### Predict for cross validation data 
  predictionValidatoin <- predict(fit, newdata=validationP)
  result <- rbind(result, c(mtry, confusionMatrix(predictionValidatoin, validationP$classe)$overall[1], "validation"))
}
### Sanitize result DF
result<-result[-c(1,2), ] 
colnames(result) = c("mtry", "accuracy", "type")
result$mtry<-as.numeric(result$mtry)
result$accuracy<-as.numeric(result$accuracy)
```
```{r, fig.width=6, fig.height=4}
print(qplot(mtry, accuracy, data=result, color=type, geom="point"))
```

As mtry = 10 achieved the best accuracy, this is taken as the final optmized model.

Analysis of training curve
==========================
Training is redone here with different numbers of training data.
```{r warning=FALSE, message=FALSE}
### Initialize result DF
result<-data.frame(c(1,1), c(1,1), c("training", "validation"))
for (sampleCount in seq(1000, nrow(trainingP), 500))
{
  ### Training the model with subsampled training data.
  filename <- paste("cache/fit-rf-samplecount-", sampleCount, ".rds", sep="")
  set.seed(4911)
  trainingSampledP <- trainingP[sample(1:nrow(trainingP), sampleCount, replace=FALSE), ]
  #fit <- train(classe ~ ., data=trainingSampledP, method="rf", tuneGrid = data.frame(.mtry = 10))
  #saveRDS(fit, file=filename)
  fit <- readRDS(filename)
  ### Predict for training data
  predictTraining <- predict(fit, newdata=trainingSampledP)
  result <- rbind(result, c(sampleCount, confusionMatrix(predictTraining, trainingSampledP$classe)$overall[1], "training"))
  ### Predict for cross validation data 
  predictionValidatoin <- predict(fit, newdata=validationP)
  result <- rbind(result, c(sampleCount, confusionMatrix(predictionValidatoin, validationP$classe)$overall[1], "validation"))
}
### Sanitize result DF
result<-result[-c(1,2), ] 
colnames(result) = c("sampleCount", "accuracy", "type")
result$sampleCount<-as.numeric(result$sampleCount)
result$accuracy<-as.numeric(result$accuracy)
```
```{r, fig.width=6, fig.height=4}
print(qplot(sampleCount, accuracy, data=result, color=type, geom="point"))
```

Accuracy becomes relatively flat after 10 000 samples and it does not meaningfully go down after that.
This means a) number of training data is enough under this model and b) the model does not overfit.

Performance Evaluation
======================
Here is the prediction to the test data with the final prediciton model.

```{r warning=FALSE, message=FALSE}
finalFit<-readRDS("cache/fit-rf-10.rds")
predictTesting <- predict(finalFit, newdata=testingP)
print(confusionMatrix(predictTesting, testingP$classe))
```

As shown, the total accuracy is 99.24% and 95% confidence level of this accuracy is (98.91%, 99.48%).
Sensitivity of class B and class D are smaller than other classes. It may be possible that collecting more data on these two claasses may increase the accuracy of this model.

Prediction of submission data
=============================
Here is the prediction to the cases for automated grading submission.
Read the data from CSV and keep only the columns as same as training data has.
Then, predict with the final model.

```{r warning=FALSE, message=FALSE}
submission <- read.csv(file="pml-testing.csv", header=TRUE, sep=",", quote="\"", na.strings=c("NA",""), as.is = T)
submission <- submission[ , colnames(submission) %in% colnames(training)]
submissionP <- predict(preprocess, submission)
predictSubmission <- predict(finalFit, newdata=submissionP)
print(predictSubmission)
```

Acknowledgment
==============
This dataset is provided by the courtesy of Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013
http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201



